```mermaid
flowchart TD
    n1["Before data processing, a data analysis plan (for your use case and context) is created for the data."] -- YES or N/A --> n2["Data collectors or processors could have given different weight to elements that confirm their beliefs or that disprove them."]
    n1 -- NO or I DON't KNOW --> n3["An assessment is done to reflect on the completeness, accuracy, collection procedures, timeliness and consistency of the data. The suitability for the current use case and context was taken into account."]
    n2 -- NO or N/A --> n4["There were a significant number of outliers or cases with missing data."]
    n3 -- YES or N/A --> n2
    n3 -- NO or I DON'T KNOW --> n5["<span style="padding-left: 8px; padding-right: 8px;">There may be data collection bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Your data may not be truly representative of the situation you are trying to investigate.</span><br>"]
    n5 -- Mitigation --> n6["<span style="padding-left: 8px; padding-right: 8px;">Before data collection&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">Create a requirements document that reflects how the data can answer your main questions, and whether the data is indeed suitable to answer them. Do the Bias Risk Assessment for Data Collection.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data collection</span><span style="padding-left: 8px; padding-right: 8px;">If not done already, do the Bias Risk Assessment for Data Collection and/or check the Data Catalog to examine the known limitations and scope of the dataset. Create a data analysis plan describing how you will organize and analyse the data, and the requirement that follow from that.</span><br>"]
    n6 -.-> n2
    n2 -- YES or I DON'T KNOW --> n7["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have confirmation bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Confirmation bias means that the data scientist’s or researcher’s beliefs influence the way data is analyzed. It is the unconsciously processing of data in ways that affirm preexisting beliefs and hypotheses (e.g. by excluding certain data points).</span><br>"]
    n7 -- Mitigation --> n8["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Confirmation bias during data processing can be minimized or prevented by having an analysis plan in place prior to the data analysis. This should include the hypotheses to be tested, how they will be tested, how missing data will be handled etc.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Make sure multiple people check your analysis. If you use a dataset that is processed by someone else, make sure you can retrace the steps and can come to the same results (e.g. handling of missing data). Strict use of a test/hold-out set can also mitigate the risk of confirmation bias. Instead of removing outliers, use the median instead of average as a closer representation of the whole data set.</span><br>"]
    n8 -.-> n4
    n4 -- NO or N/A --> n9["<span style="padding-left: 8px; padding-right: 8px;">Data from multiple sources was aggregated for analyses or the analyses were done at an averaged/group level.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">For example, there is information at a store level, but analysis is done on market level. Or, there is information about individual salaries, but average salary of the group is used in analysis.&nbsp;</span><br>"]
    n4 -- YES or I DON'T KNOW --> n10["The processing of missing data/outliers was checked for bias by multiple people or the process was duplicated by others with the same results."]
    n10 -- YES --> n9
    n10 -- NO or I DON'T KNOW --> n11["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have confirmation bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Confirmation bias means that the data scientist’s or researcher’s beliefs influence the way data is analyzed. It is the unconsciously processing of data in ways that affirm preexisting beliefs and hypotheses (e.g. by excluding certain data points).</span><br>"]
    n11 -- Mitigation --> n12["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Confirmation bias during data processing can be minimized or prevented by having an analysis plan in place prior to the data analysis. This should include the hypotheses to be tested, how they will be tested, how missing data will be handled etc.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Make sure multiple people check your analysis. If you use a dataset that is processed by someone else, make sure you can retrace the steps and can come to the same results (e.g. handling of missing data). Strict use of a test/hold-out set can also mitigate the risk of confirmation bias. Instead of removing outliers, use the median instead of average as a closer representation of the whole data set.</span><br>"]
    n12 -.-> n9
    n9 -- NO or N/A --> n13["<span style="padding-left: 8px; padding-right: 8px;">It is checked that data processing operations did not result in unbalanced (sub)groups.&nbsp;<br></span><span style="padding-left: 8px; padding-right: 8px;">For example in case of:&nbsp;<br></span><span style="padding-left: 8px; padding-right: 8px;">- de-identification&nbsp;<br></span><span style="padding-left: 8px; padding-right: 8px;">- correcting errors<br></span><span style="padding-left: 8px; padding-right: 8px;">- mitigation for missing data (e.g., imputation)<br></span><span style="padding-left: 8px; padding-right: 8px;">- removing outliersNO</span><br>"]
    n9 -- YES --> n14["It is ensured that the individual datasets or groups have a similar statistical distribution or that insights drawn from aggregated data reflect the diversity of underlying elements."]
    n14 -- YES --> n13
    n14 -- NO or I DON'T KNOW --> n15["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have aggregation bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Group level statistics are used instead of individual or lower level statistics.</span><br>"]
    n15 -- Mitigation --> n16["<span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Avoiding aggregation bias involves striking a balance between granularity and clarity, ensuring that insights drawn from aggregated data reflect the diversity of underlying elements. Bias can be substantially corrected by slicing the data so that each slice is homogeneous with respect to the outcome of interest. Consider using disaggregated data, multilevel modeling, weighted averages, and sensitivity analysis to avoid or mitigate the effects of aggregation bias.&nbsp;</span><br>"]
    n16 -.-> n13
    n13 -- YES or N/A --> n17["<span style="padding-left: 8px; padding-right: 8px;">Your data contains a good representation of your target population. i.e., your data captures a sample of the population that accurately reflects the characteristics of the larger group that is the intended target of your solution.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">(simplified example: if you build a solution for Diabetes type 2 patients, you have made sure to have included middle aged and older people in your dataset, which is the population where this disease occurs most frequently.)</span><br>"]
    n13 -- NO --> n18["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have sample bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Sample bias means the results are skewed a certain way because you’ve only captured feedback from a certain segment of your target audience or you have only captured a subset of the important influencing factors.</span><br>"]
    n18 -- Mitigation --> n19["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Selection bias can be mitigated by random sampling and sample size calculation and justification, and by having clear exclusion and inclusion criteria that consider the different impacts on different groups. For a clinical study, blinding and double-blinding reduce the risk of selection bias by preventing participants and researchers to affect the outcome.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Clustering and visualization methods on the data can help identify bias by looking at the spread of certain variables and examine whether they match the expected data distributions.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Selection bias in training data can be mitigated using data-reweighting, for instance with sampling techniques like stratified sampling (e.g. using SMOTE for synthetic minority oversampling) or by careful feature selection when some features have a strong correlation with the bias (e.g., skin colour).&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">When you resample training data, do not evaluate your model on the resampled data because the model will overfit to that resampled distribution. Instead of transforming the training data, the reweighting method attaches weight to each case such that there is statistical parity in the data with respect to the protected feature. The same effect can also be achieved by sampling cases.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">It is important to not simply exclude cases with missing values because that may lead to selection (and other) bias(es). However, use of an imputation strategy or&nbsp; data can also result in (selection or confirmation) bias. It is crucial to always conduct exploratory data analyses to check whether your dataset is representative of the population of interest, and to check whether the characteristics of the synthetic data do not differ significantly from the original data.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">&nbsp;</span><br>"]
    n19 -.-> n17 & n20["<span style="padding-left: 8px; padding-right: 8px;">Consider your outcome of interest, this is your dependent variable.</span><span style="padding-left: 8px; padding-right: 8px;">Now keep in mind the main factors that could influence that outcome directly, they are the independent variables in your data. Does your data contain a balanced sample with respect to the values of these independent variables?</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">The independent variables depend of course strongly on the use case, for instance: camera angles, background sound, lighting conditions, body position, accent, weather, income.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span>"] & n21["<span style="padding-left: 8px; padding-right: 8px;">Your data includes sufficient datapoints for the relevant minority groups (i.e., smaller sub-populations) that will be using your solution, or that are relevant to your study outcome.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">(simplified example: if you build a solution for Diabetes type 2 patients, you have made sure to include people under 45 even though they are less likely to get the disease than older people.)</span><br>"]
    n17 -- YES or N/A --> n20
    n17 -- NO or I ODN'T KNOW --> n18
    n20 -- NO or I DON'T KNOW --> n18
    n20 -- YES or N/A --> n21
    n21 -- YES or N/A --> n22["<span style="padding-left: 8px; padding-right: 8px;">The outcome of interest is considered for relevant combinations of attributes.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">For instance, if race and sex is relevant in your target group, the data was not just analyzed for male and female groups and for black and white groups, but for intersections of these attributes, such as for black females.&nbsp;</span><br>"]
    n21 -- NO or I DON'T KNOW --> n23["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have class imbalance bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">In class imbalance bias there is considerably more data available for one class over another (for instance a disparity in the number of samples corresponding to each sub-population).</span><br>"]
    n23 -- Mitigation --> n24["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">inclusive study design: utilize multiple recruitment channels, collaborate with community organizations, address barriers to participation and make sure that the study materials are inclusively formulated and designed. Create an understanding of the impacted community and engage in early, meaningful and ongoing community consultation.</span><span style="padding-left: 8px; padding-right: 8px;">Track detailed demographic information to monitor the representativeness of your sample.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Use class weights to give different weighing to different classes.</span><span style="padding-left: 8px; padding-right: 8px;">Use resampling methods like oversampling and undersampling.</span><span style="padding-left: 8px; padding-right: 8px;">Generate synthetic data.</span><span style="padding-left: 8px; padding-right: 8px;">Leverage domain knowledge to create appropriate features.</span><br>"]
    n24 -.-> n22
    n22 -- YES or N/A --> n25["A domain expert has been involved before, during or after data collection, or during data analysis."]
    n22 -- NO or I DON'T KNOW --> n26["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have intersectionality bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Intersectionality bias is a failure to consider how overlapping identities (e.g., race, gender, socioeconomic status) interact to affect individuals' experiences and outcomes. These joint distributions reveal structures in data that marginal distributions might hide, which could identify biases within subgroups.</span><br>"]
    n26 -- Mitigation --> n27["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Make sure to collect a sufficiently large sample to analyze the relevant combinations of attributes. Look at how gender identity, age, ability and/or disability status, and race and/or ethnicity could lead to a disadvantage. Look at how other groups may have an advantage.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Recognize that ensuring fairness at all levels of intersectional groups is infeasible due to potentially infinite overlapping subgroups. Analysing the data, disaggregated by intersectional attributes, will identify which groups of people should be the focus of your strategies. The specific context of the project directs the mitigation measures that can be used (for instance modifying cross-attention maps in a disentagled manner for diffusion-based text-to-image models). Some suggestions are listed in https://arxiv.org/pdf/2305.06969 .</span><br>"]
    n27 -.-> n25
    n25 -- YES or N/A --> n28["Data annotation/labeling is part of the data processing process."]
    n25 -- NO or I DON'T KNOW --> n29["<p>Your dataset may have <span style="color: rgb(218, 0, 99);">availability bias</span>.</p><p></p><p>Availability bias means that a lack of knowledge of the context in which the data has been collected and/or the study goals lead to misinterpretation by placing greater emphasis on information relevant or familiar to the data scientist/researcher.</p>"]
    n29 -- Mitigation --> n30["<p><em style="color: rgb(45, 155, 240);">Prior to Data collection:</em></p><p>Availability bias can be reduced by involving experts with knowledge of the topic in question. In addition, availability bias can be reduced by reflecting on the process and by considering whether the information and data used to make a decision was sufficient.</p>"]
    n30 -.-> n28
    n28 -- NO or N/A --> n31["There are obvious features/attributes that could link to the outcome of interest and introduce unwanted discrimination (e.g., sensitive attributes like race, weight, gender, age, religion)."]
    n28 -- YES --> n32["Before labeling, labels were clearly defined (e.g. unambiguous) and were tested for stereotypes. The labels were assigned by experts who are knowledgeable in the domain."]
    n32 -- YES or N/A --> n33["There are missing or delayed labels, or labels that are low of quality."]
    n32 -- NO or I DON'T KNOW --> n34["After labeling, interrater reliability was checked between labels that were assigned by independent labellers.<br>"]
    n34 -- YES --> n33
    n34 -- NO --> n35["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have labeling bias.<br></span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Labeling bias means that labels are incorrectly assigned, for example because the labels are not representative of the set of potential labels, or because the labels are assigned incorrectly, or because they are based on a proxy rather than the actual data of interest.&nbsp;</span><br>"]
    n35 -- Mitigation --> n36["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Use multiple human annotators (per data point) who label data independently, and make sure that they have high interrater reliability.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Do sample checks of submitted labels to check for accuracy. In case of expert human annotators, multiple annotators could evaluate. Those data points for which the labels disagree a lot might be removed from the data or be re-evaluated by a panel. In case of automated labelling, we can use semi-supervised labelling: we infer a probability that the unlabelled examples are correctly classified and feed that back into the classifier with a weight that is proportional to that probability.&nbsp;</span><br>"]
    n36 -.-> n33
    n33 -- YES or I DON'T KNOW --> n38["<p>When processing the missing or low-quality labels (e.g., removing, using synthetic data, using proxy information), it was taken into account that the frequency for those labels could be higher in one group than another.</p><p></p><p><em>(Information note: E.g., there might be less information of people who were released from a hospital than those that were not)</em></p>"]
    n33 -- NO or nN/A --> n31
    n38 -- YES --> n31
    n38 -- NO or I DON'T KNOW --> n39["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have labeling bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Labeling bias means that labels are incorrectly assigned, for example because the labels are not representative of the set of potential labels, or because the labels are assigned incorrectly, or because they are based on a proxy rather than the actual data of interest.&nbsp;</span><br>"]
    n39 -- Mitigation --> n40["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Use multiple human annotators (per data point) who label data independently, and make sure that they have high interrater reliability.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Do sample checks of submitted labels to check for accuracy. In case of expert human annotators, multiple annotators could evaluate. Those data points for which the labels disagree a lot might be removed from the data or be re-evaluated by a panel. In case of automated labelling, we can use semi-supervised labelling: we infer a probability that the unlabelled examples are correctly classified and feed that back into the classifier with a weight that is proportional to that probability.&nbsp;</span><br>"]
    n40 -.-> n31
    n31 -- NO or N/A --> n41["An active effort was made to uncover confounders, e.g. by asing domain experts of doing a literature study."]
    n31 -- YES or I DON'T KNOW --> n42["These sensitive attributes were removed for all purposes except if explicitly needed for use case or for bias detection (as stated under the AI Act article 9 for high-risk systems)."]
    n42 -- YES or N/A --> n43["It was confirmed that there were no other features that strongly correlated to these sensitive features and could thereby act as proxies."]
    n43 -- YES or N/A --> n41
    n42 -- NO or I DON'T KNOW --> n44["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may lead to discrimination (by proxy).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Discrimination means that there is an adverse effect resulting from a prejudicial approach in favour of one group over another.</span><br>"]
    n44 -- Mitigation --> n45["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Eliminate the use of and access to sensitive categories of data as much as possible, unless you are allowed to use them to check against biases under the AI act or other legislation. Have an expert examine the data collection and processing procedure and requirements of the used data points.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Closely check your data for correlation between parameters and sensitive data categories to expose potential proxies. If you have sensitive data you might be allowed to use it to check for and mitigate biases under the AI Act for high-risk systems.</span><br>"]
    n43 -- NO or I DON'T KNOW --> n44
    n13 --> n46["Untitled Node"]
    n45 -.-> n41
    n41 -- YES or N/A --> n47["<span style="padding-left: 8px; padding-right: 8px;">Known confounders are considered and included in the study design/data collection where possible.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Confounders are those variables that might influence both your dependent variable (the supposed effect) as well as the independent variable (the supposed cause).</span><br>"]
    n41 -- NO or I DON'T KNOW --> n48["<p>Your dataset may have <span style="color: rgb(218, 0, 99);">Simpson's and/or Berkson's paradox</span> <span style="color: rgb(218, 0, 99);">(also known as collider bias)</span>.</p><p></p><p>Simpson’s paradox means that trends appear in subgroups, but disappear or reverse when subgroups are combined.</p><p>Berkson’s paradox means that the independent and dependent variables are influenced by a confounder that is not included in the study design.</p>"]
    n48 -- Mitigation --> n49["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:</span><span style="padding-left: 8px; padding-right: 8px;">Explicitly determine at which level you will need to do analysis (between groups, within groups, within people), and then carefully assess whether the level at which the data were collected aligns with the explanatory level of the intended analyses. Berkson's paradox can be prevented by carefully applying appropriate inclusion criteria.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:</span><span style="padding-left: 8px; padding-right: 8px;">To detect whether your dataset has Berkson's paradox, it can be helpful to use causal diagrams (directacyclic graphs (DAGs)). DAGs are visual presentations, generalizations of conventional linear path diagrams that are nonparametric.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">To detect whether your dataset may have Simpson's paradox, it is important to conduct exploratory data analyses stratifying the sample into subgroups, e.g. stratifying samples into low- and high-incidence of a certain disease. Simpson’s paradox can be mitigated by addressing causal relations in the statistical model, e.g. use of a conditional independence test for contingency tables to check that dividing into subgroups does not yield conclusions that conflict with the conclusion based on the aggregate data, or the use of cluster analysis to detect the presence of subpopulations within a dataset based on common statistical patterns. See also Alipourfard et al 2018.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">For both paradoxes it is important to think about what confounding variables may be in the dataset and may affect the outcome. There are a number of checks that you can conduct to check for confounding, e.g:</span><span style="padding-left: 8px; padding-right: 8px;">1. If the potential confounding variable is binary, generate stratified 2 × 2 tables, and compare their results  with those in the combined table. If the separate tables have the opposite association compared to the   combined table, or if there is an association in the separate tables that is not there when the data are   combined (or vice versa), this is evidence of confounding.</span><span style="padding-left: 8px; padding-right: 8px;">2. Check for significant associations between the potential confounder and both the outcome and the factor of interest. If both associations exist, that is evidence of confounding.</span><span style="padding-left: 8px; padding-right: 8px;">3. Assess the association of interest using logistic regression models: one with just the factor of interest as a predictor, and another with the potential confounder included as a covariate. A rule of thumb states that if the odds ratio estimate for the factor of interest differs between the two models by 10% or more, you should conclude that there is confounding.</span><br>"]
    n49 -.-> n47
    n47 -- YES or N/A --> n50["<span style="padding-left: 8px; padding-right: 8px;">There could be changes in the data distribution of data compared to data that was collected earlier in time (e.g. training data).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">An example could be a change in sales channel, e.g., more people shopping online than in the store.</span><br>"]
    n47 -- NO or I DON'T KNOW --> n51["<p style="">Your dataset may have confounding bias.</p><p style="">Confounding bias means that variables outside the scope of the existing model were not controlled for but these variables mayb influence both the independent and dependent variables..</p>"]
    n51 -- Mitigation --> n52["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:</span><span style="padding-left: 8px; padding-right: 8px;">This can be mitigated by including known confounders in the study design. Unknown confounders can be partially controlled for with randomisation and having a large sample size. Also, using directed acyclic graphs can help to identify causal effects between variables.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:</span><span style="padding-left: 8px; padding-right: 8px;">Causal effect estimation techniques can help to remove confounding bias from data. For instance, using propensity scores (people with higher scores are more likely to have certain confounders, these scores are used to build a statistical model) can mitigate confounding. Directed acyclic graphs can help identify those variables that need to be controlled for in your statistic analyses. See for information this explanation.&nbsp;</span><br>"]
    n52 -.-> n50
    n50 -- NO or N/A --> n53["In this dataset, what is influencing the outcome variable of interest might have changed compared to earlier data (e.g., training data), i.e., the statistical properties of the target variable have changed over time."]
    n50 -- YES or I DON'T KNOW --> n54["It was confirmed that statistical properties of the data did not change significantly over time (e.g., when considering training data versus inference data)."]
    n54 -- YES or N/A --> n53
    n53 -- NO or N/A --> n55["There are continuous features/attributes that are translated to discrete or dichotomous categories (e.g., age or income) without analysis of whether this has impacted the detection of important patterns."]
    n53 -- YES or I DON'T KNOW --> n56["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have concept drift.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Concept drift means there are changes in the relationships between input and target variables.</span><br>"]
    n56 -- Mitigation --> n57["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Have a data quality assurance process in place that can involve techniques such as deduplication, standardization, and validation to ensure that the data is accurate and consistent over time. Make sure you can monitor the properties of your input data over time.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">For detection, four building blocks need to be in place: gathering data, modeling the data, determining a suitable score to compare distributions, and performing significance tests. There are supervised and unsupervised methods of doing this. For mitigation, the model that was created with the training data should be updated or retrained (e.g., with incremental learning, online learning, or ensemble methods).</span><br>"]
    n57 -.-> n55
    n55 -- NO or N/A --> n58["Statistical tests are used that assume normality while the errors (model residuals) might be subject to a different distribution.<br>"]
    n55 -- YES --> n59["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have discretization bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Discretization bias is the error resulting from transforming continuous data into discrete bins. This can lead to potential loss of information and misleading conclusions.</span><br>"]
    n59 -- Mitigation --> n60["<p><em style="color: rgb(45, 155, 240);">After Data Collection:</em></p><p><em>If you cannot keep the continuous variables, try using one of the following approaches: use smaller bins, use bins of varying widths that adapt to the distribution of the data (e.g. narrower bins in a range where more data is concentrated), use of smoothing techniques (e.g., kernel smoothing or moving averages), quantile binning, hierarchical binning. Alternatively, use insights from the domain to ensure alignments with meaningful categories. Visualize data before and after discretization to ensure important patterns are not lost.&nbsp;</em></p>"]
    n60 -.-> n58
    n58 -- NO or N/A --> n61["<p>End of the data processing</p>"]
    n58 -- YES or I DON'T KNOW --> n62["You are dealing with a very small sample (&lt;200 data points)"]
    n62 -- NO --> n61
    n62 -- YES --> n63["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have normality bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Normality bias is the assumption that the model residuals (the error in the relationship between the independent variables and the dependent variable) follows a normal distribution. This might result in biased estimates of group differences and inflated Type I error rates.</span><br>"]
    n63 -- Mitigation --> n64["<span style="padding-left: 8px; padding-right: 8px;">After Data Collection:</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">A number of statistical tests, parametric tests such as the Student's t-test and the one-way and two-way ANOVA, require a normally distributed sample population. You can use tests like the Kolmogorov–Smirnov test and the Shapiro–Wilk test to test for normality.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">When the distribution of the residuals is found to deviate from normality, possible solutions include transforming the data, removing outliers, or conducting an alternative analysis that does not require normality (e.g., a nonparametric regression or robust methods). Transformation of data before analysis is advisable such as as logarithmic or square root transformations.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Visual inspection for outliers and heteroscedasticity (check if the standard deviations of a variable are nonconstant) is important for assessment.</span><br>"]
    n64 -.-> n61



