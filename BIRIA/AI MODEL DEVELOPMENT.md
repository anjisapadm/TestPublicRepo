```mermaid
flowchart TD
    n1["A bias risk assessment was performed for data processing for the dataset that is used."] -- YES or N/A --> n2["The dataset is used for the purpose for which it was originally collected."]
    n1 -- NO or I DON'T KNOW --> n3["Conduct the bias risk assessment for data processing before you proceed here."]
    n3 -- PROCEED ANYWAY --> n2
    n2 -- YES or N/A --> n4["The population represented in the dataset matches the population that is the target of this AI model."]
    n2 -- NO or I DON'T KNOW --> n5["A bias risk assessment was done for both data collection and data processing with the current purpose in mind."]
    n5 -- YES or N/A --> n4
    n5 -- NO or I DON'T KNOW --> n6["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from domain shift.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Decontextualizing data raises questions related to privacy, consent, and internal validity of ML model results.</span><br>"]
    n6 -- Mitigation --> n7["<span style="padding-left: 8px; padding-right: 8px;">Conduct the bias risk assessment for data collection and data processing before you proceed. This is to check whether your data is representative, does not contain historical biases, does not improperly utilize protected attributes and does not utilize culturally/contextually unsuitable attributes.&nbsp;</span><br>"]
    n7 -.-> n4
    n4 -- YES or N/A --> n8["There is a balanced distribution of data and labels over the different classes of interest"]
    n4 -- NO or I DON'T KNOW --> n9["<span style="padding-left: 8px; padding-right: 8px;">Your model may have coverage bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Coverage bias is a type of selection bias where data is not selected in a representative way, for instance when the population represented in a dataset does not match the population that the model is making predictions about.</span><br>"]
    n9 -- Mitigation --> n10["<span style="padding-left: 8px; padding-right: 8px;">Before Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Try to capture diverse training data by collecting data from a wide variety of sources and to represent various demographics/regions/viewpoints.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">There are several strategies to mitigate the effects of coverage bias:</span><span style="padding-left: 8px; padding-right: 8px;">- Data augmentation can be used to artificially expand the training dataset. Take care not to introduce new biases through the augmentation process.</span><span style="padding-left: 8px; padding-right: 8px;">- Update your training data to reflect changes in the target population.</span><span style="padding-left: 8px; padding-right: 8px;">- (Adversarial) domain adaptation can reduce bias by allowing a classifier trained on a source domain to generalize well to a target domain (e.g., using sample-based, feature-based or inference-based methods).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span>"]
    n10 -.-> n8
    n8 -- YES or N/A --> n11["<span style="padding-left: 8px; padding-right: 8px;">It was checked that the addition of sensitive parameters does not lead to a disadvantage, for instance in decisions or outcomes of the model</span><span style="padding-left: 8px; padding-right: 8px;">(other than those that are part of the acceptance criteria, or those required for optimal care, treatment and diagnosis). Special care was taken to also consider the combinations of these attributes (i.e., intersectionality).</span><br>"]
    n8 -- NO or I DON'T KNOW --> n12["<span style="padding-left: 8px; padding-right: 8px;">Your dataset may have class imbalance bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">In class imbalance bias there is considerably more data available for one class over another (for instance a disparity in the number of samples corresponding to each sub-population).</span><br>"]
    n12 -- Mitigation --> n13["<span style="padding-left: 8px; padding-right: 8px;">Data level:<br></span><span style="padding-left: 8px; padding-right: 8px;">You can change the data distribution to make it less imbalanced by using class weights (give different weighing to different classes) or resampling methods (like oversampling and undersampling). With two-phase learning you first train on the resampled data and then fine-tune on the original data.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">The generation of synthetic data can help to create a more balanced dataset, for instance through label-preserving transformations,&nbsp; perturbations (adding noisy samples), or mixup (combining labels).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Algorithm level:<br></span><span style="padding-left: 8px; padding-right: 8px;">Make sure that you use metrics that help you understand model performance with respect to specific classes (e.g., consider using how well your model does on the 'negative' class).&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">Adjust the loss function using for example cost-sensitive learning, class-balanced loss or focal loss.&nbsp;</span><br>"]
    n13 -.-> n11
    n11 -- YES or N/A --> n14["The purpose of the model (or one of its purposes) is to do risk profiling."]
    n11 -- NO or I DON'T KNOW --> n15["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from (in)direct discrimination.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Discrimination means the less favorable treatment of one person or group compared to another in a comparable situation on basis of certain (protected) characteristics.Often this is prohibited by law, or only allowed for very 'serious reasons'.</span><br>"]
    n15 -- Mitigation --> n17["<span style="padding-left: 8px; padding-right: 8px;">Define relevant groups of interest (e.g., demographic groups, subject matter experts) within the context of use as part of plans for gathering structured public feedback. Do fundamental rights impact assessment.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Protected characteristics are allowed only in very restricted cases, make sure to check with your legal officer whether this is allowed in your case.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Statistical hypothesis testing can provide insight into whether there exists a statistically significant relationship between the profiling criterion and the aim pursued. If not, it is an ineligible criterion for profiling. Make sure to use the appropriate sample size to achieve the appropriate confidence level. See www.algorithmaudit.eu</span><br>"]
    n17 -.-> n14 & n20["The model will use nationality or race as a feature to differentiate."] & n23["Race or nationality (or a proxy thereof) is the ONLY selection criteria in the risk profile, or the risk profile is targeted on people of only ONE certain origin or nationality."] & n25["The risk profile will use a selection criterion that directly differentiates on race or nationality."] & n26["The application of the risk profile pursues a legitimate aim (e.g., public safety, prevention of criminality, anti-fraud)."] & n27["The specific risk profile is suitable for the pursued aim, necessary and proportionate."] & n18["The thresholds or boundaries of different groups/classes were aligned with a domain expert"]
    n14 -- NO or N/A --> n18
    n14 -- YES --> n19["<span style="padding-left: 8px; padding-right: 8px;">An expert evaluated the available variables to check which ones are eligible to use for profiling (including the use of statistical hypothesis testing).&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Reasons for exclusion can for instance be: legally forbidden to use, no linkage to aim, subjective or non-verifiable, or subject to change.</span><br>"]
    n19 -- NO or I DON'T KNOW --> n15
    n19 -- YES --> n20
    n20 -- YES or I DON'T KNOW --> n22["Based on the race or nationality variables (or a proxy thereof), there can be an adverse affect for some in comparison to others in a similar situation."]
    n20 -- NO --> n18
    n22 -- NO --> n23
    n22 -- YES or I DON'T KNOW --> n24["There is a direct differentiation on the basis of race or nationality in the context of social protection or social security (i.e., providing benefits to individuals on the basis of risks faced in life e.g., unemployment, disability, health)."]
    n24 -- NO --> n23
    n24 -- YES or I DON'T KNOW --> n15
    n23 -- NO --> n25
    n23 -- YES or I DON'T KNOW --> n15
    n25 -- NO --> n26
    n25 -- YES or I DON'T KNOW --> n15
    n26 -- YES --> n27
    n26 -- NO or I DON'T KNOW --> n15
    n27 -- NO or I DON'T KNOW --> n15
    n27 -- YES --> n18
    n18 -- YES or N/A --> n28["Testing of the model is carried out against prior defined metrics and acceptance criteria that are appropriate to the intended purpose of the system."]
    n28 -- YES or N/A --> n29["The modeling choices (e.g. model selection, weighing of variables, decision thresholds, how features were engineered or how parameters were tuned) were informed by best practices and expert domain knowledge.<br>"]
    n29 -- YES or N/A --> n30["More than one person has reviewed the structure and implementation of the model"]
    n30 --> n31["The process of model creation and tuning was made explicit and documented (including e.g., identified assumptions)."]
    n29 -- NO or I DON'T KNOW --> n32["The modeling choices were primarily made by one person based on the variables available in the dataset."]
    n31 -- YES or N/A --> n33["Data size is sufficient for the task and is at least the size of the required sample size. Sample size was determined based on tooling (in case of hypothesis testing) or by careful consideration of model parameters and validation requirements (in case of AI model testing)."]
    n31 -- NO or I DON'T KNOW --> n34["<span style="padding-left: 8px; padding-right: 8px;">The modeling choices may cause your model to be biased (e.g, confirmation bias).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Human choices that are involved with creating and validating a model can create bias (e.g. which features to include, what architecture to use, how to set thresholds and variable weights, defining test metrics).</span><span style="padding-left: 8px; padding-right: 8px;">Confirmation bias is when you unconsciously process data in ways that affirm preexisting beliefs and hypotheses.</span><br>"]
    n34 -- Mitigation --> n35["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:</span><span style="padding-left: 8px; padding-right: 8px;">Critically think about the data that you need to validate your hypothesis. Make sure you include domain experts and consult theory and best practices when making choices about features, thresholds, validation methods. Document them in the Algorithm Catalog.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:</span><span style="padding-left: 8px; padding-right: 8px;">Make sure you include domain experts and consult theory and best practices when making choices about features, thresholds, validation methods. Document them in the Algorithm Catalog.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">For regression problems, a regularization loss term can be added to the loss function, which becomes large when a class is determined mainly based on sensitive features. This means sensitive features become less influential in the final model Adversarial debiasing can help to overcome learned biases by modeling both a predictor and an adversary</span><br>"]
    n32 -- NO --> n30
    n32 -- YES or I DON'T KNOW --> n34
    n30 -- NO or I DON'T KNOW --> n34
    n28 -- NO or I DON'T KNOW --> n34
    n18 -- NO or I DON'T KNOW --> n34
    n35 -.-> n28 & n29 & n30 & n31 & n33
    n33 -- YES or N/A --> n36["The data has been splitted to create test, training and validation sets."]
    n33 -- NO or I DON'T KNOW --> n37["<span style="padding-left: 8px; padding-right: 8px;">Validation with hold-out (e.g. a single holdout set or K-fold cross-validation) has been used during algorithm development.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Note: a validation data set is a data set used to tune the model hyperparameters. The training data set is used to train the different models, the validation data set is used to compare model performances and decide&nbsp; which one to take and the test data set is used to obtain the&nbsp; performance characteristics (e.g. accuracy, specificity).</span><br>"]
    n37 -- YES --> n36
    n37 -- NO or I DON'T KNOW --> n38["<p style="">Your model may suffer from underfitting.</p><p style=""><br></p><p style="">Underfitting means that the algorithm is not sufficiently optimized for the data or does not adequately represent the structure of the data.</p>"] & n42["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from overfitting.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Overfitting means that the algorithm matches the data too closely and has represented some of the residual variation in the underlying model structure.</span><br>"]
    n38 -- Mitigation --> n39["<span style="padding-left: 8px; padding-right: 8px;">Both overfitting and underfitting result in the model not generalizing well from training data to new data, leading to poor performance.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Make sure that you split the data into independent training, test and validation sets. You may consider having seperate teams working on training and evaluation, to minimize bias.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">For both underfitting and overfitting data augmentation can help to create additional training values (e.g. pytorch offers great functionality for data transformations to augment data). Two-phase learning and dynamic sampling can mitigate the risk of over- and underfitting. Feature-crossing is useful to model the nonlinear relationship between features.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">In deep learning, overfitting can be mitigated by regularization: changing the complexity of the network to reduce generalization error (either by number of weights of values of the weights).&nbsp;&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Cross-validation can help to diagnose under-and overfitting.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">To overcome overfitting for predictive models, it is recommended to re-do data preparation within your cross validation folds (including&nbsp; tasks like feature selection, outlier removal, encoding, feature scaling). Alternatively, split your training dataset into train and validation sets, and store away the validation dataset. Furthermore, use data poisoning attacks/adversarial approaches for testing for bias.</span><br>"]
    n36 -- YES or I DON'T KNOW --> n41["The algorithm is/will be tested on a testset that is independent of the training data set, after model training is completed."]
    n36 -- NO or N/A --> n43["There are (likely to be) missing features, or features that are low of quality."]
    n41 -- NO or I DON'T KNOW --> n42
    n41 -- YES or N/A --> n44["<span style="padding-left: 8px; padding-right: 8px;">The data splits were made before scaling or augmenting the data.The statistics from the train split were used to scale or augment all the splits.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">In case of cross-validation the parameters for rescaling data were used within each fold of the cross validation and those parameters were used to for the test fold on each cycle.</span><br>"]
    n42 -- Mitigation --> n39
    n39 -.-> n36 & n43
    n44 -- YES or N/A --> n45["<span style="padding-left: 8px; padding-right: 8px;">Data was splitted by time, rather than randomly.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">E.g., if you have data from five weeks, first four weeks were used for training and week five was split randomly into validation and test.&nbsp;</span><br>"]
    n44 -- NO or I DON'T KNOW --> n47["<span style="padding-left: 8px; padding-right: 8px;">Your model may have data leakage.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Data leakage occurs when a form of the label 'leaks' into the set of features used for making predications, and this information is not available during inference.</span><br>"]
    n45 -- YES or N/A --> n46["Strongly correlated data points are divided into different splits (e.g., CT scans of a person taken closely together in time, or multiple photos of the same object)."]
    n45 -- NO or I DON'T KNOW --> n47
    n46 --> n43
    n46 -- NO or I DON'T KNOW --> n47
    n47 -- Mitigation --> n48["<span style="padding-left: 8px; padding-right: 8px;">Data leakage can happen during generation, collection, sampling, splitting and processing data or feature engineering. Be sure to monitor for leakage during the entire lifecycle of the project.&nbsp;&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">- Measure the predictive power of each feature (set) with respect to the target variable (label). If it has unusually high correlation, investigate whether this makes sense.&nbsp;<br></span><span style="padding-left: 8px; padding-right: 8px;">- Do ablation studies to measure how important a feature (combination) is, and investigate very important features.&nbsp;<br></span><span style="padding-left: 8px; padding-right: 8px;">- If a new features significantly improves model performance, check that it does not contain leaked information about labels.&nbsp;<br></span><span style="padding-left: 8px; padding-right: 8px;">- Don't look at the test set other than to report model's final performance.&nbsp;</span><br>"]
    n48 -.-> n45 & n46
    n43 -- NO or N/A --> n49["Feature scaling was done to create similar importance for the features."]
    n43 -- YES or I DON'T KNOW --> n50["<p>When processing the missing features (e.g., removing, using synthetic data, using proxy information), it was taken into account that the frequency for those missing features could be higher in one group than another.</p><p></p><p><em>(Information note: E.g., the patient history for certain groups of people might be less complete due to more fragmented care they received)</em></p>"]
    n50 -- YES or N/A --> n49
    n50 -- NO or I DON'T KNOW --> n51["<p>Your model may have <span style="color: rgb(218, 0, 99);">feature bias. </span></p><p></p><p>In case of feature bias there is an uneven distribution of features in training dataset, or a feature has different meanings or implications across different groups.</p>"]
    n51 -- Mitigation --> n52["<p><em style="color: rgb(45, 155, 240);">After Data Collection:</em></p><p>Explainability techniques can help to identify which features might introduce bias (e.g., LIME, SHAP). You could introduce new features that explicitly looks at the interactions of features.</p><p>You might consider creating different models for the different groups. Alternatively another model type might cope better with the feature bias.</p><p>Feature Importance Analysis (FIA) can help to identify and remove less important features to reduce noise and improve the signal for the impacted class. Example methods are Iterative Orthogonal Feature Projection (IOFP) or Minimum Redundancy, Maximum Relevance (MRMR).</p><p>Feature Distribution Smoothing (FDS) is a method to perform distribution smoothing on the feature space to calibrate biased estimates, which is especially helpful for underrepresented target values in the training data.</p>"]
    n52 -.-> n49
    n49 -- YES or N/A --> n53["During model development and testing, correlations between input and output have been checked for bias and stereotypes (e.g. using feature attribution)"]
    n49 -- NO or I DON'T KNOW --> n54["<p>Your model may have <span style="color: rgb(218, 0, 99);">feature scaling bias. </span></p><p></p><p>In case of scaling bias, the importance of the different features varies based on the range of their values (instead of their contribution to the outcome).</p>"]
    n54 -- Mitigation --> n55["<p></p><p><em style="color: rgb(45, 155, 240);">After Data Collection:</em></p><p>Scale features to be in the same range, e.g. using techniques like standardization or log transformation. (Take care not to introduce data leakage by making sure scaling occurs after the data is split into train/test/validation sets. The training data needs to be scaled based on the values in the training set. Testing set needs to be scaled based on the parameters used to scale the training data.)</p>"]
    n55 -.-> n53
    n53 -- YES or N/A --> n56["Known confounders are included in the collection process and/or were controlled for during analysis or development."]
    n53 -- NO or I DON'T KNOW --> n57["<span style="padding-left: 8px; padding-right: 8px;">Either<br></span><span style="padding-left: 8px; padding-right: 8px;">(i) Sensitive parameters (e.g. gender, ethnicity, socio-economic backgrounds) are, or could be, used to obtain model output or<br></span><span style="padding-left: 8px; padding-right: 8px;">(ii) There are factors/variables in the model that could (unintentionally) be correlated with sensitive parameters (i.e., serve as proxies for sensitive parameters)</span><br>"]
    n57 -- NO or N/A --> n56
    n57 -- YES or I DON'T KNOW --> n58["<p style="">Your model may have latent bias.</p><p style="">Latent bias means that the algorithm (incorrectly) correlates concepts with other concepts based on existing bias in data.</p>"]
    n58 -- Mitigation --> n59["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:</span><span style="padding-left: 8px; padding-right: 8px;">Make sure the data is a balanced sample. Note that even if the sample is representative, latent bias can still occur. Sometimes, datasets are available that are corrected for latent bias (e.g. word embeddings in NLP are prone to learn human stereotypes and prejudices, one can search for debiased word embeddings or word embeddings that have clear bias analyses.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:</span><span style="padding-left: 8px; padding-right: 8px;">Closely examine the training data to discover the latent structure present in the data. If the latent structure is found, reweighing can help to resample the data in order to get a more balanced and debiased dataset.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">In domains such as medicine, it can be acceptable for machine learning models to include sensitive attributes such as gender and ethnicity.&nbsp; In this approach, a separate model is trained for each group (decoupled) such that they satisfy preference guarantees (group model performs better than a common model that ignores sensitive attributes).</span><br>"]
    n59 -.-> n56
    n56 -- YES or N/A --> n60["The connection between factors that affect the algorithm's decisions is easy to identify and makes sense (for example, the relationship between inputs/features and expected outcomes).<br>"]
    n56 -- NO or I DON'T KNOW --> n61["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from simplicity bias (aka shortcut learning).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Simplicity bias means that your model over-depends on simple, weakly predictive features, rather than the more robust but complexer features. Often a correlation-implies-causation fallacy is at the root of this.</span><br>"]
    n61 -- Mitigation --> n62["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Use randomised, controlled experiments whenever possible.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Check whether effects can be predicted by another variable. Check for interaction/moderation effects or mediating variables. Look at theory for interpretation of models.</span><span style="padding-left: 8px; padding-right: 8px;">Feature disentanglement can be of help. Directed acyclic graphs can help identify relations between variables that need to be controlled for. Causal calculus or structure learning can be used to identify causal effects.</span><span style="padding-left: 8px; padding-right: 8px;">Sstatistical methods like stratification, regression analysis and propensity score can be used to control confounding and create a more balanced dataset.</span><br>"]
    n62 -.-> n60
    n60 -- YES or N/A --> n63["<span style="padding-left: 8px; padding-right: 8px;">A definition of fairness was defined for this context, with corresponding appropriate fairness metrics.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Which metrics of fairness should be used depends on the use/business case and should be decided together with the stakeholders. Examples of metrics can be found here.</span><br>"]
    n60 -- NO or I DON'T KNOW --> n64["Unknown confounders are controlled for, for instance with randomization and by testing on out of distribution data."]
    n64 -- YES or N/A --> n63
    n64 -- NO or I DON'T KNOW --> n65["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from simplicity bias (aka shortcut learning).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Simplicity bias means that your model over-depends on simple, weakly predictive features, rather than the more robust but complexer features. Often a correlation-implies-causation fallacy is at the root of this.</span><br>"]
    n65 -- Mitigation --> n66["<p><em style="color: rgb(45, 155, 240);">Prior to Data collection:</em></p><p>Use randomised, controlled experiments whenever possible.</p><p></p><p><em style="color: rgb(45, 155, 240);">After Data Collection:</em></p><p>Check whether effects can be predicted by another variable. Check for interaction/moderation effects or mediating variables. Look at theory for interpretation of models.</p><p><span style="color: rgb(26, 26, 26);">Feature disentanglement can be of help. Directed acyclic graphs can help identify relations between variables that need to be controlled for. Causal calculus or structure learning can be used to identify causal effects.</span></p><p>Sstatistical methods like stratification, regression analysis and propensity score can be used to control confounding and create a more balanced dataset.</p>"]
    n66 -.-> n63
    n63 -- YES or N/A --> n67["The algorithm was evaluated using fairness metrics. Errors are distributed similarly across relevant demographics and output quality is comparable for different subpopulations (e.g. based on age, gender, race, disability, language etc) in the dataset."]
    n63 -- NO or I DON'T KNOW --> n70["<span style="padding-left: 8px; padding-right: 8px;">Your model may have a biased loss function.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Biased loss function means that the algorithm was optimized for success which usually means maximizing accuracy or efficiency rather than maximizing fairness. This can result in performance disparities.&nbsp;</span><br>"]
    n67 -- YES or N/A --> n68["Changes were made for specific groups to ensure fair outcomes without fully understanding the source of bias or whether the correction addressed that specific cause."]
    n67 -- NO or I DON'T KNOW --> n70
    n68 -- NO or N/A --> n69["Different models could be developed that solve the problem equally well (i.e., we can think of other models or model implementations that could provide the same results for the testing data)."]
    n68 -- YES or I DON'T KNOW --> n70
    n70 -- Mitigation --> n71["<span style="padding-left: 8px; padding-right: 8px;">Prior to Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Define relevant groups of interest (e.g., demographic groups, subject matter experts) within the context of use as part of plans for gathering structured public feedback. Do a fundamental rights impact assessment.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">It is important to make sure that your loss function captures all elements of your goal, for instance by adding a term for the fairness metric of interest. Also, you could test/validate your algorithm on a validation set that includes a lot of edge cases.</span><span style="padding-left: 8px; padding-right: 8px;">Reweighting explicitly instructs the loss function to penalize the misclassification of certain samples more harshly. The prejudice removal regularizer method can help to make sensitive features less influential in the final model.</span><span style="padding-left: 8px; padding-right: 8px;">Use of adversarial methods can help the system to learn a task in a way that is mindful of the biases that you are trying to prevent.</span><span style="padding-left: 8px; padding-right: 8px;">Make sure to check performance of your model on different data slices (e.g., split on important dimensions for your solution for instance platform, location, age).</span><br>"]
    n71 -.-> n67 & n69 & n68
    n69 -- NO or N/A --> n72["What type of AI model do you use?"]
    n69 -- YES or I DON'T KNOW --> n73["<p>You may have <span style="color: rgb(218, 0, 99);">underspecified</span> your model.</p><p></p><p>The available testing data can be equally well-matched by different configurations of the modelâ€™s computational structure, so it does not guarantee predictable, equivalent real-world performance.</p><p></p><p>Underspecification defines the inability of the pipeline to identify whether these models have embedded the structure of the underlying system, and hence it is unable to assess the degree to which the models will be generalizable.</p>"]
    n73 -- Mitigation --> n74["<span style="padding-left: 8px; padding-right: 8px;">After Data Collection:<br></span><span style="padding-left: 8px; padding-right: 8px;">Thoroughly test models on application-specific tasks, and in particular to check that the performance on these tasks is stable.</span><span style="padding-left: 8px; padding-right: 8px;">Explore loss landscapes.</span><br>"]
    n74 -.-> n72
    n72 -- Traditional/discriminative AI --> n75["<span style="padding-left: 8px; padding-right: 8px;">End of the bias risk assessment for model development.</span><br>"]
    n72 -- Generative AI --> n76["The model is trained on synthetic data that is generated by (earlier versions of) itself or other genAI models."]
    n76 -- NO or N/A --> n77["The output of the model is emphasizing WEIRD (Western, Educated, Industrialized, Rich, Democratic) perspectives or&nbsp; stereotypical output (e.g., with respect to race or gender)."]
    n76 -- YES or I DON'T KNOW --> n78["The synthetic training data is balanced with high-quality human data."]
    n78 -- YES or N/A --> n77
    n78 -- NO or I DON'T KNOW --> n79["<p>Your model may suffer from <span style="color: rgb(218, 0, 99);">model collapse</span>.</p><p></p><p>Model collapse occurs when the model outputs over time become less diverse and fail to capture the complexity of the original data. In early model collapse, the model loses information about the tails of the data distribution (often minority data). In late model collapse, the model blends patterns in the data together.</p>"]
    n77 -- NO or N/A --> n75
    n77 -- YES or I DON'T KNOW --> n81["<p>Your <span style="background-color: transparent;">model</span> may<span style="color: rgb(218, 0, 99);"> amplify stereotypes.</span></p><p></p><p>Generative AI models are known to produce content that disproportionately reflect regressive stereotypes (for instance related to gender, skin color, occupation) present in the training data.</p>"]
    n79 -- Mitigation --> n80["<p><em style="color: rgb(45, 155, 240);">Before Data Collection:</em></p><p>Try to include a wide range of data during training to present the model with variation.</p><p></p><p><em style="color: rgb(45, 155, 240);">After Data Collection:</em></p><p>Add noise or regularization to the generator and discriminator, this introduces randomness and variation.</p><p>Using mini-batch discrimination enables the discriminator in a GAN to look at multiple outputs from the generator at once, rather than one at a time.</p><p>Use multiple discriminators and generators.</p><p>Include a differentiable metric that can measure the diversity and realism of the generated samples, and incorporate it into the loss function of the generator.</p><p>Retain a 'prestige' copy of the original, human-produced dataset, and periodically refreshing the model with it.</p><p>Monitor for collapse, check that your data is not overly homogenous.</p>"]
    n80 -.-> n77
    n81 -- Mitigation --> n82["<p><span style="color: rgb(45, 155, 240);">After Data Collection</span>:</p><p>Gather input from stakeholder communities to determine what output is considered acceptable and unacceptable. Continuously monitor the output and set up alerts for high-risk output in real-time. Use appropriate benchmarks (such as Bias Benchmark Questions, RealHateful or Harmful Prompts), to identify systemic bias, stereotyping, denigration, and hateful content in the system's outputs. Allocate time and resources for outreach, feedback, and recourse processes during the development and deployment of the model.</p>"]
    n82 -.-> n75



