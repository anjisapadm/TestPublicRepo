```mermaid
flowchart TD
    n1["<span style="padding-left: 8px; padding-right: 8px;">The model is applied in a different context than the one it was trained for, or the context may have changed during deployment.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Note: For example, consider a model that was trained on medical images from one hospital and is deployed in another hospital</span><br>"] -- NO or N//A --> n2["The model is proven to be effective and to generate high-quality output for the population/context relevant to the current deployment. It was tested with different types of end users and user experience across a broad spectrum of real-world scenarios were taken into account."]
    n1 -- YES or I DON'T KNOW --> n3["<p>Training data has been compared to data from the new context, and steps have been taken to match those datasets (or create new ones) and retrain and reevaluate the performance.</p><p></p><p><em>Note: changing the dataset can increase other types of bias (see risk assessment for data collection)</em></p>"]
    n3 -- YES or N/A --> n2
    n3 -- NO or I DON'T KNOW --> n4["<p style="padding: 20px; width: 800px;"><span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from data drift (covariate shift)domain shift (context bias).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Data drift is a change in the statistical properties and characteristics of the input data (i.e., the data distribution). This occurs when a model is in production and new data deviates from either the data the model trained on or earlier production data.</span><br>"]
    n4 -- Mitigation --> n5["<p style="padding: 20px; width: 800px;"><span style="padding-left: 8px; padding-right: 8px;">Prior to deployment:&nbsp;<br></span><span style="padding-left: 8px; padding-right: 8px;">Involving experts with knowledge of the context in question can help to make the model robust against data drift (ideally this is done before data collection). Make sure that features that affect certain groups are present in the training data. You can introduce randomness in system outputs to create more robust training data.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Domain generzalization can be used to augment training sets for instance with domain shift tackling augmentations (e.g. addition of synthetic data similar to the shifted domain). In case generalization is not sufficient, domain adaptation can be realized by fine-tuning the model on (labelled) data from the new domain.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Weigh the importance of input data such that is inversely proportional to the age of the data (i.e. more attention is paid to the most recent data).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Providing data and model cards with the data will help to create awareness about the context in which data was collected and the model was trained, including limitations of application.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After deployment:<br></span><span style="padding-left: 8px; padding-right: 8px;">Monitor performance continuously. Create quantifiable performance metrics to assess model performance and ensure you measure changes on a granular level.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Ideally, models are robust to both in-domain and out of domain samples.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">For in-domain you can use detection by Univariate Measure (e.g. Kolmogorov-Smirnov or Cramer's V) to measure the difference between older and newer samples of a variable. The out-of-domain cases should be detected with a domain classifier (using either density-based methods, reconstruction-based methods, distance-based methods, or classification-based methods).&nbsp;&nbsp;</span><br>"]
    n5 -.-> n2
    n2 -- YES or N/A --> n6["The model implementation deviates from the one that was validated, or the model was purchased from another party."]
    n2 -- NO or I DON'T KNOW --> n7["The model will be tested on new users (from a diverse population relevant to the context) before deployment."]
    n7 -- YES or N/A --> n6
    n7 -- NO or I DON'T KNOW --> n8["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from data drift (covariate shift)domain shift (context bias).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Data drift is a change in the statistical properties and characteristics of the input data (i.e., the data stribution). This occurs when a model is in production and new data deviates from either the data the model trained on or earlier production data.</span><br>"]
    n8 -- Mitigation --> n9["<p style="padding: 20px; width: 800px;"><span style="padding-left: 8px; padding-right: 8px;">Prior to deployment:&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">Involving experts with knowledge of the context in question can help to make the model robust against data drift (ideally this is done before data collection). Make sure that features that affect certain groups are present in the training data. You can introduce randomness in system outputs to create more robust training data.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Domain generzalization can be used to augment training sets for instance with domain shift tackling augmentations (e.g. addition of synthetic data similar to the shifted domain). In case generalization is not sufficient, domain adaptation can be realized by fine-tuning the model on (labelled) data from the new domain.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Weigh the importance of input data such that is inversely proportional to the age of the data (i.e. more attention is paid to the most recent data).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Providing data and model cards with the data will help to create awareness about the context in which data was collected and the model was trained, including limitations of application.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After deployment:</span><span style="padding-left: 8px; padding-right: 8px;">Monitor performance continuously. Create quantifiable performance metrics to assess model performance and ensure you measure changes on a granular level.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Ideally, models are robust to both in-domain and out of domain samples.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">For in-domain you can use detection by Univariate Measure (e.g. Kolmogorov-Smirnov or Cramer's V) to measure the difference between older and newer samples of a variable. The out-of-domain cases should be detected with a domain classifier (using either density-based methods, reconstruction-based methods, distance-based methods, or classification-based methods).&nbsp;&nbsp;</span><br>"]
    n9 -.-> n6
    n6 -- NO or N/A --> n10["When the model is deployed, monitoring is in place to compare newer to older data, seeing whether they are derived from the same underlying distribution"]
    n6 -- YES or I DON'T KNOW --> n11["The algorithm was re-evaluated using Philips training/test data or relevant benchmarks."]
    n11 -- YES or N/A --> n10
    n11 -- NO or I DON'T KNOW --> n12["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from data drift (covariate shift)domain shift (context bias).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Data drift is a change in the statistical properties and characteristics of the input data (i.e., the data stribution). This occurs when a model is in production and new data deviates from either the data the model trained on or earlier production data.</span><br>"]
    n12 -- Mitigation --> n13["<p style="padding: 20px; width: 800px;"><span style="padding-left: 8px; padding-right: 8px;">Prior to deployment:&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">Involving experts with knowledge of the context in question can help to make the model robust against data drift (ideally this is done before data collection). Make sure that features that affect certain groups are present in the training data. You can introduce randomness in system outputs to create more robust training data.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Domain generzalization can be used to augment training sets for instance with domain shift tackling augmentations (e.g. addition of synthetic data similar to the shifted domain). In case generalization is not sufficient, domain adaptation can be realized by fine-tuning the model on (labelled) data from the new domain.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Weigh the importance of input data such that is inversely proportional to the age of the data (i.e. more attention is paid to the most recent data).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Providing data and model cards with the data will help to create awareness about the context in which data was collected and the model was trained, including limitations of application.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After deployment:</span><span style="padding-left: 8px; padding-right: 8px;">Monitor performance continuously. Create quantifiable performance metrics to assess model performance and ensure you measure changes on a granular level.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Ideally, models are robust to both in-domain and out of domain samples.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">For in-domain you can use detection by Univariate Measure (e.g. Kolmogorov-Smirnov or Cramer's V) to measure the difference between older and newer samples of a variable. The out-of-domain cases should be detected with a domain classifier (using either density-based methods, reconstruction-based methods, distance-based methods, or classification-based methods).&nbsp;&nbsp;</span><br>"]
    n13 -.-> n10
    n10 -- YES or N/A --> n14["Accuracy metrics and fairness metrics are monitored over time to check for deviations or error while in production."]
    n10 -- NO or I DON'T KNOW --> n15["<span style="padding-left: 8px; padding-right: 8px;">our model may suffer from data drift (covariate shift)domain shift (context bias).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Data drift is a change in the statistical properties and characteristics of the input data (i.e., the data stribution). This occurs when a model is in production and new data deviates from either the data the model trained on or earlier production data.&nbsp;</span><br>"]
    n15 -- Mitigation --> n16["<p style="padding: 20px; width: 800px;"><span style="padding-left: 8px; padding-right: 8px;">Prior to deployment:&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">Involving experts with knowledge of the context in question can help to make the model robust against data drift (ideally this is done before data collection). Make sure that features that affect certain groups are present in the training data. You can introduce randomness in system outputs to create more robust training data.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Domain generzalization can be used to augment training sets for instance with domain shift tackling augmentations (e.g. addition of synthetic data similar to the shifted domain). In case generalization is not sufficient, domain adaptation can be realized by fine-tuning the model on (labelled) data from the new domain.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Weigh the importance of input data such that is inversely proportional to the age of the data (i.e. more attention is paid to the most recent data).</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Providing data and model cards with the data will help to create awareness about the context in which data was collected and the model was trained, including limitations of application.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After deployment:</span><span style="padding-left: 8px; padding-right: 8px;">Monitor performance continuously. Create quantifiable performance metrics to assess model performance and ensure you measure changes on a granular level.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Ideally, models are robust to both in-domain and out of domain samples.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;">For in-domain you can use detection by Univariate Measure (e.g. Kolmogorov-Smirnov or Cramer's V) to measure the difference between older and newer samples of a variable. The out-of-domain cases should be detected with a domain classifier (using either density-based methods, reconstruction-based methods, distance-based methods, or classification-based methods).&nbsp;&nbsp;</span><br>"]
    n16 -.-> n14
    n14 -- YES or N/A --> n17["The model continuously learns and is updated, or the provider might change the model while it is in use."]
    n14 -- NO or I DON'T KNOW --> n18["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from concept drift.&nbsp;&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Concept drift occurs when there is a change in the underlying relationship between input data (features) and the target variable over time. This means that the real-world conditions that generate the data have shifted while data might look similar. Concept drift can be caused by sudden, gradual or recurrent drift.&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span>"]
    n18 -- Mitigation --> n19["<p style="padding: 20px; width: 800px;"><span style="padding-left: 8px; padding-right: 8px;">Before deployment:&nbsp;<br></span><span style="padding-left: 8px; padding-right: 8px;">When labeling data, divide the data stream into a series of windows and assign a class label to individual data points based on the business context.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Ensemble learning with model weighting might be more robust as the output is generally a weighted average across output of multiple models.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After deployment:<br></span><span style="padding-left: 8px; padding-right: 8px;">Make an effort to distinguish between noise and drift.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Monitor the performance of the model over time. If model performance is declining this could be a signal of concept drift. For streaming data, ADWIN (ADaptive WINdowing) could be useful, for batched data, the Kolmogorov-Smirnov test or adversarial validation.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Monitoring the confidence of your output can also help shed light on concept drift, for instance when there is a shift in confidence scores over time.</span><br>"]
    n19 -.-> n17
    n17 -- NO or N/A --> n20["There is a mechanism that allows others/users to flag issues related to bias, discrimination, or inappropriate behavior of the AI system"]
    n17 -- YES or I DON'T KNOW --> n21["The model is updated based on the output of (other or the same) genAI models"]
    n21 -- NO or N/A --> n20
    n21 -- YES or I DON'T KNOW --> n22["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from (late) model collapse.&nbsp;&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">In late model collapse, the model converges to a distribution that has little resemblance to the original distribution, often with substantially reduced variety in output, amplification of biases or little connection to the real world.</span><br>"]
    n22 -- Mitigation --> n23["<p><span style="color: rgb(45, 155, 240);">Before deployment: </span></p><p>Create diverse training data. If your dataset contains synthetic data, make sure that it is balanced with human/real-world data. Augment synthetic data to accurately represent complexities of the context or problem. You can add noise to create a wider range of patterns.</p><p></p><p><span style="color: rgb(45, 155, 240);">After deployment:</span></p><p>Create monitor mechanisms that allow for oversight of the generated outputs (model 'health checks').</p><p></p><p>Incorporate user feedback loops and ensure that human-generated content is included in the training data. Make sure that the data contains a broad representation of patterns and styles.</p><p></p><p>Note that the impact of model collapse is most acute with data that is less common because the output looses data that is further from the most common patterns.</p>"]
    n23 -.-> n20
    n20 -- YES or N/A --> n24["The model generates output that is subject to norms and constraints with respect to how this model can (or should) behave."]
    n20 -- NO or I DON'T KNOW --> n25["<span style="padding-left: 8px; padding-right: 8px;">Your model lacks controls for detecting wrong or inappropriate output.&nbsp;&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Your model might create output outside of the pre-defined scope of what is acceptable and desirable, such as discriminating, stereotypical, or toxic output.&nbsp;</span><br>"]
    n25 -- Mitigation --> n26["<p style="padding: 20px; width: 800px;"><span style="color: rgb(45, 155, 240);">Before deployment: </span></p><p>Careful data curation and content moderation. Ensure diverse data input.</p><p>Check for biases in input data and mitigate (e.g., filter out toxic language, make sure there is a balanced representation of inputs). Include curated datasets of acceptable behavior, such as examples of how the model should respond to irresponsible requests.</p><p>Implement adversarial techniques or create a team (ideally with representatives of the target population) to deliberately (elicit undesired responses or decisions (e.g., prompt hacking).</p><p>Generate multiple outputs from the same AI model, and rank them on appropriateness.</p><p>Investigate and document past system incidents and failure modes.</p><p><span style="color: rgb(45, 155, 240);">After deployment:</span></p><p>Implement another AI model to check the outputs of your first AI model for inappropriate content.</p><p>Enable user feedback and monitor user response to flag inappropriate output. Seek active and direct feedback from potentially affected communities via structured feedback mechanisms or red-teaming to monitor and improve outputs.</p><p>Establish human oversight to check outputs before they are released or shared with the user.</p><p>Use strong meta-prompt that acknowledge diverse populations and instruct models to avoid stereotyping, denigrating or toxic outputs.</p>"]
    n24 -- NO or N/A --> n27["There are specific limits and restrictions on how this model can (or should) be used."]
    n24 -- YES or I DON'T KNOW --> n28["There are guardrails in place that ensure high-quality, appropriate output."]
    n28 -- NO or I DON'T KNOW --> n25
    n28 -- YES or N/A --> n27
    n27 -- YES or N/A --> n29["The model could impact (actions or decisions influencing) the health of people."]
    n27 -- NO or I DON'T KNOW --> n30["There are clear instructions or a UI that has been tested and validated to result in proper use of the solution"]
    n26 -.-> n27
    n30 -- YES or N/A --> n29
    n30 -- NO or I DON'T KNOW --> n31["<span style="padding-left: 8px; padding-right: 8px;">Your model may suffer from misuse or inappropriate use.&nbsp;&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">If your model is used incorrectly or outside of anticipated contexts, many biases might result from that (e.g., selection bias, contextual bias, measurement bias) and it could lead to ethical or moral issues.</span><br>"]
    n31 -- Mitigation --> n32["<p style="padding: 20px; width: 800px;"><span style="color: rgb(45, 155, 240);">Before deployment: </span></p><p>Create clear instructions for interaction and applicability that have been tested and validated to result in proper use of the solution.</p><p>Work on an interface that enforces certain limitations and applicability.</p><p></p><p>Fine-tune your models on your task specifically so as to refine its capabilities for the intended context.</p><p></p><p>Document clearly described limitations and applicability domain of the solution in the Algorithm Catalog.</p><p></p><p><span style="color: rgb(45, 155, 240);">After deployment:</span></p><p>Provide&nbsp;clear, concise, and specific instructions with the model (e.g., as part of a Model Card or the Algorithm Catalog).</p><p>Provide pre-approved responses for common or foreseeably problematic inputs.</p><p>Monitor user input and flag inappropriate cases that need to be handled by humans or should not be processed by the model.</p><p></p>"]
    n32 -.-> n29
    n29 -- NO or N/A --> n33["External validity testing has shown that this model's performance results can be reasonably generalized to all foreseeable groups that can be positively or negatively impacted."]
    n29 -- YES or I DON'T KNOW --> n34["The model is or will be validated with appropriate rigor (e.g., with prospective data, in a randomized controlled trial)."]
    n34 -- YES or N/A --> n33
    n33 -- YES or N/A --> n35["The model is or will be validated in the workflow or context (e.g. using A/B tests or evaluated by experts)"]
    n34 -- NO or I DON'T KNOW --> n36["<span style="padding-left: 8px; padding-right: 8px;">Your model may have validation bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">The model is not validated sufficiently and is assumed to generalize or work based on limited evidence, e.g. the model is evaluated based on ground truth data only, inappropriate samples or inappropriate methodology are used, or it has not been tested in actual workflow.</span><br>"]
    n33 -- NO or I DON'T KNOW --> n36
    n35 -- NO or I DON'T KNOW --> n36
    n36 -- Mitigation --> n37["<p style="padding: 20px; width: 800px;"><span style="padding-left: 8px; padding-right: 8px;">Prior to deployment:&nbsp;</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">In case your model is designed to work in a medical context and influence the health or quality of life of people, a Randomized Controlled Trial should be performed to evaluate and validate the model.</span><span style="padding-left: 8px; padding-right: 8px;">In other cases, well-controlled A/B tests, evaluation of human-AI configurations through structured experiments, or internal validity testing should provide evidence that the model provides robust outcomes in the contexts with different subgroups of the target population. External validity testing can include demographic data that was originally excluded.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">After deployment:<br></span><span style="padding-left: 8px; padding-right: 8px;">Apply explainable AI techniques (e.g., analysis of embeddings, model</span><span style="padding-left: 8px; padding-right: 8px;">compression/distillation, gradient-based attributions, occlusion/term reduction, counterfactual prompts, word clouds) as part of ongoing continuous improvement processes.</span><span style="padding-left: 8px; padding-right: 8px;">Carefully consider whether the model is performing well for your use case and population. In particular consider edge cases and make an assessment of the impact of different types of errors on different groups (false negatives and true positives).</span><br>"]
    n37 -.-> n33 & n35 & n38["Testing with a broad spectrum of end-users has been done to evaluate user experience and to examine the influence of the user input on system performance."]
    n35 -- YES or N/A --> n38
    n38 -- YES or N/A --> n39["Is there a possibility to leave a class unlabeled or an outcome as 'unknown' if the confidence of the system is below a certain threshold?"]
    n38 -- NO or I DON'T KNOW --> n40["<span style="padding-left: 8px; padding-right: 8px;">Your solution may have interaction bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Interaction bias means that the solution leads to a biased outcome due to the way the user interacts with it.</span><br>"]
    n40 -- Mitigation --> n41["<p><span style="color: rgb(45, 155, 240);">Before deployment</span></p><p>It is difficult to check for interaction bias as it usually is a bias that exist in your user base. It is important to apply continuous checks on your model and its output, so make sure that you are able to track user interactions with the AI solution and that you are able to measure out-of-bounds for new incoming data.</p><p></p><p><span style="color: rgb(45, 155, 240);">After deployment:</span></p><p></p><p>Provide users with a clear instruction guideline for use of the algorithm. For adaptive systems, the outcome of the system should continuously be monitored using performance and fairness metrics (e.g. Using Fairlearn or IBMfairness360). Using out of distribution detection with confidence could be used to monitor any potential outliers.</p>"]
    n41 -.-> n39
    n39 -- YES or N/A --> n42["The outcomes of the models are presented with a probability or degree of confidence."]
    n39 -- NO or I DON'T KNOW --> n43["<span style="padding-left: 8px; padding-right: 8px;">Your model may increase complacency or automation bias.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Automation bias means that humans are over-relying on automated systems and favor decisions from the system even if there is contradictory information. This can lead to errors of commission (following incorrect advice) and omission (failing to act because of not being prompted to do so).</span><span style="padding-left: 8px; padding-right: 8px;">Complacency is a state of reduced attention where people no longer actively monitor processes or routines but rather trust the system without verification.</span><br>"]
    n43 -- Mitigation --> n44["<p style="padding: 20px; width: 800px;"><span style="color: rgb(45, 155, 240);">Before deployment:</span></p><p><span style="color: rgb(26, 26, 26);">Build in awareness checks for the user, priming them to contemplate or evaluate the algorithms output.  </span></p><p><span style="color: rgb(26, 26, 26);">Consider to give certainty estimates with the model output. </span></p><p><span style="color: rgb(26, 26, 26);">Create explainable AI that prov﻿ide&nbsp;transparancy&nbsp;about decisions and probabilities.</span></p><p></p><p><span style="color: rgb(45, 155, 240);">After deployment:</span></p><p><span style="color: rgb(26, 26, 26);">Trigger user to re-evaluate goals and commitment. </span></p><p><span style="color: rgb(26, 26, 26);">Consider to inform the user about limitations and potential biases. &nbsp;</span></p><p><span style="color: rgb(26, 26, 26);">Monitor agreement between the user's decision and the model's advice and use this to make improvements.</span></p>"]
    n42 -- NO or I DON'T KNOW --> n43
    n44 -.-> n42
    n42 -- YES or N/A --> n45["<p>Decisions that follow from the model may reinforce the model assumptions</p><p></p><p>E.g. decision to not treat patients because they have short life expectancy might affect their life expectancy. Ranking options and putting A on top make users click option A more, reinforcing the assumption that this was the better option.</p>"]
    n45 -- NO or N/A --> n46["End of the Model deployment.<br>"]
    n45 -- YES or I DON'T KNOW --> n47["<span style="padding-left: 8px; padding-right: 8px;">Your algorithm may have degenerate feedback loops.</span><span style="padding-left: 8px; padding-right: 8px;"><br></span><span style="padding-left: 8px; padding-right: 8px;">Degenerate feedback loops means that decisions that follow from the model reinforce the assumed relations that drove that decision, resulting in self-fulfilling feedback loops.</span><br>"]
    n47 -- Mitigation --> n48["<p style="padding: 20px; width: 800px;"><span style="color: rgb(45, 155, 240);">Before deployment</span></p><p><span style="color: rgb(26, 26, 26);">Think about the population that you are serving with the AI solution, and how decision making based on the algorithm could impact new incoming data. Are your model outputs reinforcing model assumptions?</span></p><p>Measure the importance of features in your model to see what drives predictions or outcomes.</p><p></p><p><span style="color: rgb(45, 155, 240);">After deployment: </span></p><p><span style="color: rgb(26, 26, 26);">Try to ensure that there is at least a feedback loop, focus on flagging or corre<span class="ql-cursor">﻿</span>cting wrong suggestions/outcomes, which are fed back to the system/engineers. </span></p><p><span style="color: rgb(26, 26, 26);">Make sure you can detect if the systems outputs are becoming more homogeneous over time, which is a sign of degenerate feedback loops. Explore techniques that can introduce randomness or find 'unbiased values' (e.g. for recommendations) with acceptable prediction accuracy loss. You can use positional features to investigate the effect of ranking or use two models that alternate. </span></p><p>Evaluate the outcomes of your model for different splits of data where you split on a variable of interest (e.g., popularity of items, severity of illness).</p><p><span style="color: rgb(26, 26, 26);">Evaluate the outcomes for the data that was not included in your system (e.g. devices that were not returned, patients that were not submitted) and are at risk of survivor bias.</span></p>"]
    n48 -.-> n46




